# AutoCrawler

AutoCrawler æ˜¯ä¸€ä¸ª Apache2 è®¸å¯çš„åˆ†å¸ƒå¼çš„å¿«é€Ÿé«˜çº§ç½‘ç»œçˆ¬è™«å’Œç½‘é¡µæŠ“å–æ¡†æ¶ï¼Œå¯ä»¥é€šè¿‡å¤§è¯­è¨€æ¨¡å‹å¿«é€Ÿèšç„¦ç½‘é¡µä¸»é¢˜ï¼ŒæŠ“å–æ•°æ®å†…å®¹ã€‚

å½“å‰é¡¹ç›®æ­£å¤„äºå¼€å‘é˜¶æ®µï¼Œä»¥ä¸‹ä»‹ç»æ˜¯åŸºäºé¡¹ç›®çš„é¢„æœŸåŠŸèƒ½,å¹¶ä¸ä»£è¡¨å·²ç»å®ç°çš„åŠŸèƒ½ã€‚

---

## ç›®å½•
* [é¡¹ç›®è¿›åº¦](README.md#é¡¹ç›®è¿›åº¦)
* [é¢„è®¡åŠŸèƒ½](README.md#é¢„è®¡åŠŸèƒ½)
* [å·¥ä½œæµç¨‹](README.md#å·¥ä½œæµç¨‹)
* [é¡¹ç›®æ¶æ„](README.md#é¡¹ç›®æ¶æ„)
* [è§£å†³çš„ç—›ç‚¹](README.md#è§£å†³çš„ç—›ç‚¹)
* [å¿«é€Ÿä¸Šæ‰‹](README.md#å¿«é€Ÿä¸Šæ‰‹)
    * [1. ç¯å¢ƒé…ç½®](README.md#1-ç¯å¢ƒé…ç½®)
    * [2. å·¥ä½œæµå¼€å‘](README.md#2-å·¥ä½œæµå¼€å‘)
    * [3. åˆå§‹åŒ–é…ç½®æ–‡ä»¶](README.md#3-åˆå§‹åŒ–é…ç½®æ–‡ä»¶)
    * [4. ä¸€é”®å¯åŠ¨](README.md#4-ä¸€é”®å¯åŠ¨)


## é¡¹ç›®è¿›åº¦

âœ…å®Œæˆè¯·æ±‚æ¨¡å—

âœ…åˆæ­¥å®Œæˆè§£ææ¨¡å—ï¼Œåç»­ä¼šé›†æˆå¤§è¯­è¨€æ¨¡å‹

âœ…åˆæ­¥å®Œæˆå·¥ä½œæµæ¨¡æ¿ï¼Œåç»­ä¼šä¸æ–­ç²¾ç®€å¹¶è®©ç”¨æˆ·æ›´æ–¹ä¾¿çš„ä½¿ç”¨

âœ…å®Œæˆæ—¥å¿—è®°å½•æ¨¡å—

âŒ›ï¸ å®Œæˆè°ƒåº¦å™¨æ¨¡å—ï¼Œç”¨äºè°ƒåº¦å·¥ä½œæµ

âŒ›ï¸ å®Œæˆæ•°æ®å¤„ç†æ¨¡å—

âŒ›ï¸ å®Œæˆæ•°æ®å­˜å‚¨æ¨¡å—

âŒ›ï¸ å®Œæˆå‰ç«¯ç®¡ç†æ¨¡å—

âŒ›ï¸ å®ŒæˆèŠ‚ç‚¹ç®¡ç†æ¨¡å—

......


## é¢„è®¡åŠŸèƒ½

ğŸ¤–ï¸ AutoCrawleræä¾›å„ç§å®ç”¨çš„å·¥å…·ï¼Œä¾‹å¦‚é€šè¿‡å¤§è¯­è¨€æ¨¡å‹æå–ç½‘é¡µurlæˆ–text

ğŸ’¡ å½“å·¥ä½œæµç¼–å†™å®Œæ¯•åå¯ä»¥æŠ•å…¥è°ƒåº¦å™¨è¿è¡Œï¼Œè°ƒåº¦å™¨é‡‡ç”¨åˆ†å¸ƒå¼æ¡†æ¶celeryã€‚å½“å·¥ä½œæµæäº¤åï¼ŒceleryèŠ‚ç‚¹ä¼šè‡ªåŠ¨è·å–ä»»åŠ¡å¹¶æ‰§è¡Œã€‚

âœ… æœ¬é¡¹ç›®è¿˜ä¼šæä¾›ç½‘é¡µç«¯è¿›è¡Œç®¡ç†ä»»åŠ¡åˆ†é…ï¼Œä»»åŠ¡ç›‘æ§ï¼Œä»»åŠ¡æ—¥å¿—æŸ¥çœ‹ç­‰åŠŸèƒ½ã€‚

ğŸ“Š é¡¹ç›®æä¾›å¤šç§æ•°æ®å­˜å‚¨æ–¹å¼ï¼ŒåŒ…æ‹¬æ•°æ®åº“å­˜å‚¨ï¼Œæ–‡ä»¶å­˜å‚¨ï¼ŒESå­˜å‚¨ç­‰ã€‚

## å·¥ä½œæµç¨‹
ç”¨æˆ·æäº¤å·¥ä½œæµåï¼Œschedulerä¼šå¯åŠ¨è¿™ä¸ªå·¥ä½œæµï¼ŒåŒæ—¶å¼€å¯ä¸€ä¸ªçº¿ç¨‹ç­‰å¾…å·¥ä½œæµçš„å®Œæˆã€‚
å½“å·¥ä½œæµå®Œæˆåï¼Œç­‰å¾…çº¿ç¨‹ä¼šå°†å·¥ä½œæµçš„ç»“æœé€šè¿‡ç®¡é“ä¼ é€’ç»™è°ƒåº¦å™¨ï¼Œè°ƒåº¦å™¨ä¼šå°†ç»“æœå­˜å‚¨åˆ°æ•°æ®å¤„ç†æ¨¡å—ä¸­ã€‚
ç®¡é“ä¸­å­˜åœ¨å¯è¢«ç”¨æˆ·å®šä¹‰çš„æ•°æ®å¤„ç†æ¨¡å—ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡ç»§æ‰¿`Workflow`ç±»ï¼Œå®ç°`data_processing`æ–¹æ³•ï¼Œå¯¹æ•°æ®è¿›è¡Œå¤„ç†ã€‚æˆ–è€…é›†æˆå¤§è¯­è¨€æ¨¡å‹ä»¥ä¸€å®šçš„è§„åˆ™å¤„ç†è¿™äº›æ•°æ®

è°ƒåº¦å™¨æä¾›æ¥å£ç”±å‰ç«¯è°ƒç”¨ï¼Œå‰ç«¯å¯ä»¥é€šè¿‡æ¥å£æŸ¥çœ‹ä»»åŠ¡çš„çŠ¶æ€ï¼Œä»»åŠ¡çš„æ—¥å¿—ï¼Œä»»åŠ¡çš„ç»“æœç­‰ã€‚

## é¡¹ç›®æ¶æ„
```shell
my_spider_project/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ endpoints.py               # å®šä¹‰APIç«¯ç‚¹ï¼ŒåŒ…å«è¿è¡Œæ—¶ä¿¡æ¯ç­‰
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ settings.py                # å…¨å±€é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ logging_config.py          # æ—¥å¿—é…ç½®
â”œâ”€â”€ crawler/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ scheduler.py               # è°ƒåº¦æ¨¡å—
â”‚   â”œâ”€â”€ requester.py               # è¯·æ±‚æ¨¡å—
â”‚   â”œâ”€â”€ parser.py                  # è§£ææ¨¡å—
â”‚   â”œâ”€â”€ workflow.py                # ç”¨æˆ·å·¥ä½œæµå¼•æ“
â”‚   â”‚â”€â”€ workflow_example.py        # å·¥ä½œæµç¤ºä¾‹
â”‚   â”‚â”€â”€ recorder.py                # å·¥ä½œæµè®°å½•å™¨
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_processor.py          # æ•°æ®å¤„ç†
â”‚   â”œâ”€â”€ storage.py                 # æ•°æ®å­˜å‚¨æ¨¡å—
â”œâ”€â”€ plugins/                       # æ’ä»¶ç›®å½•ï¼Œç”¨äºæ—¥åæ‰©å±•
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ text_extractor.py          # æ–‡æœ¬æå–æ’ä»¶
â”‚   â”œâ”€â”€ img_extractor.py           # å›¾ç‰‡æå–æ’ä»¶ï¼ˆæœªæ¥æ‰©å±•ï¼‰
â”‚   â”œâ”€â”€ video_extractor.py         # è§†é¢‘æå–æ’ä»¶ï¼ˆæœªæ¥æ‰©å±•ï¼‰
â”œâ”€â”€ workers/                       # åˆ†å¸ƒå¼èŠ‚ç‚¹ç®¡ç†
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ master_node.py             # ä¸»èŠ‚ç‚¹æ¨¡å—
â”‚   â”œâ”€â”€ worker_node.py             # ä»èŠ‚ç‚¹æ¨¡å—
â”œâ”€â”€ tests/                         # æµ‹è¯•ç›®å½•
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_api.py                # APIæ¨¡å—æµ‹è¯•
â”‚   â”œâ”€â”€ test_scheduler.py          # è°ƒåº¦æ¨¡å—æµ‹è¯•
â”‚   â”œâ”€â”€ test_requester.py          # è¯·æ±‚æ¨¡å—æµ‹è¯•
â”‚   â”œâ”€â”€ test_parser.py             # è§£ææ¨¡å—æµ‹è¯•
â”‚   â”œâ”€â”€ test_data_processor.py     # æ•°æ®å¤„ç†æ¨¡å—æµ‹è¯•
â”‚   â”œâ”€â”€ test_storage.py            # æ•°æ®å­˜å‚¨æ¨¡å—æµ‹è¯•
â”œâ”€â”€ scripts/                       # è„šæœ¬ç›®å½•ï¼Œç”¨äºå­˜æ”¾å¯åŠ¨è„šæœ¬å’Œå·¥å…·
â”‚   â”œâ”€â”€ run_worker.sh              # å¯åŠ¨ä»èŠ‚ç‚¹è„šæœ¬
â”œâ”€â”€ static/                        # é™æ€èµ„æºç›®å½•ï¼ˆå¦‚é…ç½®æ–‡ä»¶æˆ–æµ‹è¯•æ•°æ®ï¼‰
â”‚   â”œâ”€â”€ sample_config.yaml         # ç¤ºä¾‹é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ sample_data.json           # ç¤ºä¾‹æ•°æ®
â”œâ”€â”€ Dockerfile                     # Dockeré…ç½®æ–‡ä»¶
â”œâ”€â”€ docker-compose.yml             # Docker Composeæ–‡ä»¶
â”œâ”€â”€ requirements.txt               # Pythonä¾èµ–åŒ…æ–‡ä»¶
â”œâ”€â”€ README.md                      # é¡¹ç›®è¯´æ˜æ–‡æ¡£
```

## è§£å†³çš„ç—›ç‚¹

- ç½‘é¡µå…ƒç´ å®šä½éº»çƒ¦ï¼Œä¸”ä¸€ä¸ªçˆ¬è™«åªèƒ½é’ˆå¯¹ä¸€ä¸ªç½‘ç«™ï¼Œå¯ä»¥é€šè¿‡è‡ªç„¶è¯­è¨€çš„æ–¹å¼å¿«é€ŸæŠ“å–ç½‘é¡µå†…å®¹
- å¹¶å‘çˆ¬è™«æ•°æ®å¤„ç†éº»çƒ¦ï¼Œæœ¬é¡¹ç›®æä¾›åˆ†å¸ƒå¼æ¡†æ¶celeryï¼Œå¯ä»¥å¿«é€Ÿéƒ¨ç½²çˆ¬è™«ä»»åŠ¡ï¼Œå¹¶ä¸”æœ‰ç»Ÿä¸€çš„æ•°æ®å¤„ç†æ–¹å¼
- å¼€å‘ä¸åŒçˆ¬è™«éœ€è¦é‡å¤å¼€å‘ï¼Œæœ¬é¡¹ç›®æä¾›äº†ä¸€å¥—é€šç”¨çš„çˆ¬è™«æ¡†æ¶ï¼Œå¯ä»¥å¿«é€Ÿå¼€å‘ä¸åŒçš„çˆ¬è™«ä»»åŠ¡

## å¿«é€Ÿä¸Šæ‰‹

### 1. ç¯å¢ƒé…ç½®

+ é¦–å…ˆï¼Œç¡®ä¿ä½ çš„æœºå™¨å®‰è£…äº† Python 3.8 - 3.11 (æˆ‘ä»¬å¼ºçƒˆæ¨èä½¿ç”¨ Python3.11)ã€‚

```
$ python --version
Python 3.11.7
```

æ¥ç€ï¼Œåˆ›å»ºä¸€ä¸ªè™šæ‹Ÿç¯å¢ƒï¼Œå¹¶åœ¨è™šæ‹Ÿç¯å¢ƒå†…å®‰è£…é¡¹ç›®çš„ä¾èµ–

```shell

# æ‹‰å–ä»“åº“
$ git clone https://github.com/Rain-kl/AutoCrawler.git

# è¿›å…¥ç›®å½•
$ cd AutoCrawler

# å®‰è£…å…¨éƒ¨ä¾èµ–
$ pip install -r requirements.txt 

```


### 2ï¼Œ å·¥ä½œæµå¼€å‘

è¿›å…¥`crawler`æ–‡ä»¶å¤¹ä¸­ï¼Œå‚ç…§`workflow_example.py`ç¼–å†™è‡ªå·±çš„å·¥ä½œæµ

```python

class WorkflowExample(Workflow):
    def __init__(self, domain: str, start_path: str, end_path_regex: str):
        super().__init__(domain, start_path, end_path_regex)

    @register_crawler
    def main(self) -> celery.result.AsyncResult:
        print(f"Start crawling from: {self.domain + self.start_path}")
        param = self.param_base.model_copy(
            update={
                'tag': 'chapter',
                'url_path': self.start_path
            }
        )
        return step1.delay(param)

    def data_processing(self, data):
        ...
```
é¦–å…ˆç»§æ‰¿`Workflow`ç±»ï¼Œç„¶åå®ç°`main`æ–¹æ³•ï¼Œ`main`æ–¹æ³•æ˜¯å·¥ä½œæµçš„å…¥å£ï¼Œ
é€šè¿‡`register_crawler`è£…é¥°å™¨æ³¨å†Œçˆ¬è™«ä»»åŠ¡ã€‚`main`æ–¹æ³•è¿”å›ä¸€ä¸ª`celery`ä»»åŠ¡å¯¹è±¡ã€‚å½“è¿™ä¸ªä»»åŠ¡æ ‡è®°å®Œæˆæ—¶ï¼Œè¡¨ç¤ºä»»åŠ¡åˆ†å‘å®Œæ¯•


### 3. åˆå§‹åŒ–é…ç½®æ–‡ä»¶
ä¿®æ”¹é…ç½®è¯·ä¿®æ”¹.envæ–‡ä»¶

```shell
$ cp .env.example .env
```


### 4. å¯åŠ¨çˆ¬è™«

- æŒ‰ç…§ä»¥ä¸‹å‘½ä»¤å¯åŠ¨åˆ†å¸ƒå¼èŠ‚ç‚¹

```shell
$ python scripts/run_worker.py

â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸ¤— celery worker is ready!  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚                                                               â”‚
â”‚  flower url            http://0.0.0.0:5555                    â”‚
â”‚  Task Queue            redis                                  â”‚
â”‚  Celery Include        crawler.myWorkflow                     â”‚
â”‚  Redis URL             redis://localhost:6379/0               â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

```

- æŒ‰ç…§ä»¥ä¸‹å‘½ä»¤å¯åŠ¨æœåŠ¡

```shell
$ python main.py
```



## æ„Ÿè°¢ä»¥ä¸‹é¡¹ç›®
[![Readme Card](https://github-readme-stats.vercel.app/api/pin/?username=celery&repo=celery)](https://github.com/celery/celery)

[![Readme Card](https://github-readme-stats.vercel.app/api/pin/?username=mher&repo=flower)](https://github.com/mher/flower)

[![Readme Card](https://github-readme-stats.vercel.app/api/pin/?username=fastapi&repo=fastapi)](https://github.com/fastapi/fastapi)


